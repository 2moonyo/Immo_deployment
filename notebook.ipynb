{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started.\n",
      "Income Data Columns: Index(['AARTSELAAR', '30.562', 'Unnamed: 2', '51.1319 4.3827', 'Aartselaar'], dtype='object')\n",
      "Zipcode Data Columns: Index(['Postcode', 'NAME', 'SUBMUNICIPALITY', 'MAIN MUNICIPALITY',\n",
      "       'Provincie'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/n2m_wdk56vqcbt6h62ljwtjc0000gn/T/ipykernel_14557/2529494967.py:166: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  property_data[\"buildingState\"] = property_data[\"buildingState\"].replace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property data exported to: /Users/irisvirus/Desktop/Becode/Python/Projects/Deployment/Immo_deployment/utils/exports/properties_data_cleaned_20241213_141135.csv\n",
      "Property data exported to: /Users/irisvirus/Desktop/Becode/Python/Projects/Deployment/Immo_deployment/utils/exports/properties_data_cleaned_20241213_141135.csv\n",
      "Model training started...\n",
      "0:\tlearn: 161013.4038385\ttest: 158585.4976450\tbest: 158585.4976450 (0)\ttotal: 8.15ms\tremaining: 12.2s\n",
      "100:\tlearn: 94750.8279059\ttest: 91115.8783529\tbest: 91115.8783529 (100)\ttotal: 535ms\tremaining: 7.42s\n",
      "200:\tlearn: 84719.5118196\ttest: 83582.2299521\tbest: 83582.2299521 (200)\ttotal: 1.07s\tremaining: 6.91s\n",
      "300:\tlearn: 80269.3291566\ttest: 81224.3839525\tbest: 81222.0678531 (299)\ttotal: 1.78s\tremaining: 7.1s\n",
      "400:\tlearn: 77710.9304618\ttest: 80318.0250214\tbest: 80309.3983294 (391)\ttotal: 2.34s\tremaining: 6.42s\n",
      "500:\tlearn: 75753.8137354\ttest: 79907.9051594\tbest: 79907.9051594 (500)\ttotal: 2.88s\tremaining: 5.75s\n",
      "600:\tlearn: 74045.6397364\ttest: 79608.5833854\tbest: 79608.5833854 (600)\ttotal: 3.41s\tremaining: 5.1s\n",
      "700:\tlearn: 72549.3793064\ttest: 79257.0653605\tbest: 79253.4178008 (699)\ttotal: 3.92s\tremaining: 4.47s\n",
      "800:\tlearn: 71153.3043846\ttest: 79134.2699061\tbest: 79134.2699061 (800)\ttotal: 4.44s\tremaining: 3.87s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 79099.752\n",
      "bestIteration = 810\n",
      "\n",
      "Shrink model to first 811 iterations.\n",
      "0:\tlearn: 161376.2823022\ttest: 156297.1591738\tbest: 156297.1591738 (0)\ttotal: 6.81ms\tremaining: 10.2s\n",
      "100:\tlearn: 93372.6097546\ttest: 91994.4268856\tbest: 91994.4268856 (100)\ttotal: 542ms\tremaining: 7.51s\n",
      "200:\tlearn: 84113.9537068\ttest: 85241.7346128\tbest: 85241.7346128 (200)\ttotal: 1.07s\tremaining: 6.91s\n",
      "300:\tlearn: 79377.5459486\ttest: 82835.1149435\tbest: 82835.1149435 (300)\ttotal: 1.65s\tremaining: 6.57s\n",
      "400:\tlearn: 76723.3251081\ttest: 81929.6853889\tbest: 81929.6853889 (400)\ttotal: 2.19s\tremaining: 6s\n",
      "500:\tlearn: 74503.2397347\ttest: 81317.8713777\tbest: 81317.8713777 (500)\ttotal: 2.72s\tremaining: 5.42s\n",
      "600:\tlearn: 72797.5528008\ttest: 81122.0709471\tbest: 81105.0904756 (591)\ttotal: 3.25s\tremaining: 4.87s\n",
      "700:\tlearn: 71269.8850537\ttest: 80907.7932020\tbest: 80901.8939842 (697)\ttotal: 3.79s\tremaining: 4.32s\n",
      "800:\tlearn: 69913.0281084\ttest: 80746.9430279\tbest: 80746.9430279 (800)\ttotal: 4.42s\tremaining: 3.85s\n",
      "900:\tlearn: 68714.5682009\ttest: 80644.8318867\tbest: 80644.8318867 (900)\ttotal: 4.95s\tremaining: 3.29s\n",
      "1000:\tlearn: 67656.2800883\ttest: 80530.2501125\tbest: 80525.7556998 (996)\ttotal: 5.49s\tremaining: 2.74s\n",
      "1100:\tlearn: 66669.0391539\ttest: 80434.9970611\tbest: 80433.3986467 (1099)\ttotal: 6.02s\tremaining: 2.18s\n",
      "1200:\tlearn: 65821.0729053\ttest: 80392.0466162\tbest: 80391.5273300 (1199)\ttotal: 6.57s\tremaining: 1.64s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 80379.56222\n",
      "bestIteration = 1224\n",
      "\n",
      "Shrink model to first 1225 iterations.\n",
      "0:\tlearn: 160436.0574204\ttest: 161666.8909559\tbest: 161666.8909559 (0)\ttotal: 7.97ms\tremaining: 12s\n",
      "100:\tlearn: 94410.7734251\ttest: 94965.0225093\tbest: 94965.0225093 (100)\ttotal: 698ms\tremaining: 9.67s\n",
      "200:\tlearn: 84347.8987334\ttest: 87632.6013347\tbest: 87632.6013347 (200)\ttotal: 1.22s\tremaining: 7.87s\n",
      "300:\tlearn: 79184.4489219\ttest: 85228.9695795\tbest: 85228.9695795 (300)\ttotal: 1.74s\tremaining: 6.93s\n",
      "400:\tlearn: 76339.8508568\ttest: 84200.4958135\tbest: 84200.4958135 (400)\ttotal: 2.27s\tremaining: 6.22s\n",
      "500:\tlearn: 74111.2471516\ttest: 83787.8360680\tbest: 83787.8360680 (500)\ttotal: 2.85s\tremaining: 5.68s\n",
      "600:\tlearn: 72085.9156717\ttest: 83379.2897074\tbest: 83375.7003128 (598)\ttotal: 3.38s\tremaining: 5.06s\n",
      "700:\tlearn: 70451.9506395\ttest: 83171.4777355\tbest: 83169.6320661 (682)\ttotal: 3.91s\tremaining: 4.46s\n",
      "800:\tlearn: 69071.1702625\ttest: 82938.5376636\tbest: 82922.8638333 (793)\ttotal: 4.42s\tremaining: 3.86s\n",
      "900:\tlearn: 67934.7500870\ttest: 82844.5320885\tbest: 82836.7192772 (894)\ttotal: 5.02s\tremaining: 3.34s\n",
      "1000:\tlearn: 66821.2338958\ttest: 82758.7873975\tbest: 82734.0366606 (991)\ttotal: 5.53s\tremaining: 2.76s\n",
      "1100:\tlearn: 65759.4985313\ttest: 82660.5219573\tbest: 82644.1402778 (1096)\ttotal: 6.13s\tremaining: 2.22s\n",
      "1200:\tlearn: 64778.1680598\ttest: 82531.6641503\tbest: 82531.6641503 (1200)\ttotal: 6.71s\tremaining: 1.67s\n",
      "1300:\tlearn: 63882.9289210\ttest: 82432.0964539\tbest: 82432.0964539 (1300)\ttotal: 7.46s\tremaining: 1.14s\n",
      "1400:\tlearn: 63061.8114733\ttest: 82329.4293233\tbest: 82328.8330027 (1399)\ttotal: 7.99s\tremaining: 564ms\n",
      "1499:\tlearn: 62294.7500483\ttest: 82258.0069476\tbest: 82253.0811021 (1489)\ttotal: 8.51s\tremaining: 0us\n",
      "\n",
      "bestTest = 82253.0811\n",
      "bestIteration = 1489\n",
      "\n",
      "Shrink model to first 1490 iterations.\n",
      "0:\tlearn: 160449.2375600\ttest: 163313.8202544\tbest: 163313.8202544 (0)\ttotal: 8.9ms\tremaining: 13.3s\n",
      "100:\tlearn: 93279.1272575\ttest: 95498.3245267\tbest: 95498.3245267 (100)\ttotal: 513ms\tremaining: 7.11s\n",
      "200:\tlearn: 84089.0647369\ttest: 87627.2272479\tbest: 87627.2272479 (200)\ttotal: 1.07s\tremaining: 6.89s\n",
      "300:\tlearn: 79302.9471264\ttest: 84943.6287011\tbest: 84943.6287011 (300)\ttotal: 1.63s\tremaining: 6.49s\n",
      "400:\tlearn: 76386.2957452\ttest: 83838.2369967\tbest: 83838.2369967 (400)\ttotal: 2.18s\tremaining: 5.97s\n",
      "500:\tlearn: 74009.3863518\ttest: 83015.2007079\tbest: 83015.2007079 (500)\ttotal: 2.79s\tremaining: 5.56s\n",
      "600:\tlearn: 71994.5575714\ttest: 82714.2568987\tbest: 82712.8994214 (599)\ttotal: 3.35s\tremaining: 5.01s\n",
      "700:\tlearn: 70281.7680202\ttest: 82511.5678950\tbest: 82500.8674553 (699)\ttotal: 3.94s\tremaining: 4.49s\n",
      "800:\tlearn: 68843.9785676\ttest: 82277.0497709\tbest: 82277.0497709 (800)\ttotal: 4.53s\tremaining: 3.95s\n",
      "900:\tlearn: 67695.1090542\ttest: 82184.7440979\tbest: 82177.9092572 (893)\ttotal: 5.09s\tremaining: 3.38s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 82137.9179\n",
      "bestIteration = 928\n",
      "\n",
      "Shrink model to first 929 iterations.\n",
      "0:\tlearn: 160382.2346493\ttest: 164129.7211684\tbest: 164129.7211684 (0)\ttotal: 7.23ms\tremaining: 10.8s\n",
      "100:\tlearn: 92546.8442189\ttest: 94936.7423814\tbest: 94936.7423814 (100)\ttotal: 612ms\tremaining: 8.47s\n",
      "200:\tlearn: 83545.4834560\ttest: 87448.2794752\tbest: 87448.2794752 (200)\ttotal: 1.15s\tremaining: 7.4s\n",
      "300:\tlearn: 79072.9267355\ttest: 84611.2158055\tbest: 84611.2158055 (300)\ttotal: 1.77s\tremaining: 7.04s\n",
      "400:\tlearn: 76672.7711317\ttest: 83424.6116885\tbest: 83424.6116885 (400)\ttotal: 2.3s\tremaining: 6.31s\n",
      "500:\tlearn: 74584.8524119\ttest: 82545.4238927\tbest: 82532.6722238 (498)\ttotal: 2.83s\tremaining: 5.65s\n",
      "600:\tlearn: 72716.1247914\ttest: 81859.6113656\tbest: 81859.6113656 (600)\ttotal: 3.39s\tremaining: 5.08s\n",
      "700:\tlearn: 71334.7956900\ttest: 81489.3144463\tbest: 81481.1454675 (695)\ttotal: 4.07s\tremaining: 4.64s\n",
      "800:\tlearn: 69861.1062549\ttest: 81106.8595283\tbest: 81103.4184255 (795)\ttotal: 4.62s\tremaining: 4.03s\n",
      "900:\tlearn: 68716.3017593\ttest: 80986.7415309\tbest: 80974.3015451 (893)\ttotal: 5.28s\tremaining: 3.51s\n",
      "1000:\tlearn: 67712.0264832\ttest: 80923.2072582\tbest: 80912.7263122 (986)\ttotal: 5.82s\tremaining: 2.9s\n",
      "1100:\tlearn: 66690.4651814\ttest: 80807.0920829\tbest: 80807.0920829 (1100)\ttotal: 6.47s\tremaining: 2.35s\n",
      "1200:\tlearn: 65804.1843188\ttest: 80759.1182842\tbest: 80738.3129365 (1183)\ttotal: 7.03s\tremaining: 1.75s\n",
      "1300:\tlearn: 64830.8531624\ttest: 80616.5662713\tbest: 80607.5660285 (1276)\ttotal: 7.66s\tremaining: 1.17s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 80541.19339\n",
      "bestIteration = 1327\n",
      "\n",
      "Shrink model to first 1328 iterations.\n",
      "Model training completed in 35.29 seconds.\n",
      "Evaluation started...\n",
      "SHAP Summary Plot (mean split) saved to shap_outputs/shap_summary_plot_mean.png\n",
      "SHAP Summary Plot (gain-based split) saved to shap_outputs/shap_summary_plot_gain.png\n",
      "SHAP Dependence Plot (mean split) for bedrooms saved to shap_outputs/shap_dependence_mean_bedrooms.png\n",
      "SHAP Dependence Plot (mean split) for locality saved to shap_outputs/shap_dependence_mean_locality.png\n",
      "SHAP Dependence Plot (mean split) for facades saved to shap_outputs/shap_dependence_mean_facades.png\n",
      "SHAP Dependence Plot (mean split) for buildingState saved to shap_outputs/shap_dependence_mean_buildingState.png\n",
      "SHAP Dependence Plot (mean split) for gardenSurface saved to shap_outputs/shap_dependence_mean_gardenSurface.png\n",
      "SHAP Dependence Plot (mean split) for pool saved to shap_outputs/shap_dependence_mean_pool.png\n",
      "SHAP Dependence Plot (mean split) for livingArea saved to shap_outputs/shap_dependence_mean_livingArea.png\n",
      "SHAP Dependence Plot (mean split) for surfaceOfThePlot saved to shap_outputs/shap_dependence_mean_surfaceOfThePlot.png\n",
      "SHAP Dependence Plot (mean split) for energy_certificate saved to shap_outputs/shap_dependence_mean_energy_certificate.png\n",
      "SHAP Dependence Plot (mean split) for median_income saved to shap_outputs/shap_dependence_mean_median_income.png\n",
      "SHAP Dependence Plot (mean split) for latitude saved to shap_outputs/shap_dependence_mean_latitude.png\n",
      "SHAP Dependence Plot (mean split) for longitude saved to shap_outputs/shap_dependence_mean_longitude.png\n",
      "SHAP Dependence Plot (mean split) for region saved to shap_outputs/shap_dependence_mean_region.png\n",
      "SHAP Dependence Plot (gain-based split) for livingArea saved to shap_outputs/shap_dependence_gain_livingArea.png\n",
      "SHAP Dependence Plot (gain-based split) for locality saved to shap_outputs/shap_dependence_gain_locality.png\n",
      "SHAP Dependence Plot (gain-based split) for latitude saved to shap_outputs/shap_dependence_gain_latitude.png\n",
      "SHAP Dependence Plot (gain-based split) for longitude saved to shap_outputs/shap_dependence_gain_longitude.png\n",
      "SHAP Dependence Plot (gain-based split) for surfaceOfThePlot saved to shap_outputs/shap_dependence_gain_surfaceOfThePlot.png\n",
      "SHAP Interaction Plot (mean split) for region vs longitude saved to shap_outputs/shap_interaction_mean_region_vs_longitude.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irisvirus/Desktop/Becode/Python/Projects/Deployment/.venv/lib/python3.12/site-packages/shap/plots/_scatter.py:579: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = pl.figure(figsize=figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP Interaction Plot (mean split) for region vs surfaceOfThePlot saved to shap_outputs/shap_interaction_mean_region_vs_surfaceOfThePlot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/n2m_wdk56vqcbt6h62ljwtjc0000gn/T/ipykernel_14557/2529494967.py:436: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(12, 8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP Interaction Plot (mean split) for region vs energy_certificate saved to shap_outputs/shap_interaction_mean_region_vs_energy_certificate.png\n",
      "SHAP Interaction Plot (mean split) for region vs livingArea saved to shap_outputs/shap_interaction_mean_region_vs_livingArea.png\n",
      "SHAP Interaction Plot (mean split) for longitude vs region saved to shap_outputs/shap_interaction_mean_longitude_vs_region.png\n",
      "SHAP Interaction Plot (mean split) for longitude vs surfaceOfThePlot saved to shap_outputs/shap_interaction_mean_longitude_vs_surfaceOfThePlot.png\n",
      "SHAP Interaction Plot (mean split) for longitude vs energy_certificate saved to shap_outputs/shap_interaction_mean_longitude_vs_energy_certificate.png\n",
      "SHAP Interaction Plot (mean split) for longitude vs livingArea saved to shap_outputs/shap_interaction_mean_longitude_vs_livingArea.png\n",
      "SHAP Interaction Plot (mean split) for surfaceOfThePlot vs region saved to shap_outputs/shap_interaction_mean_surfaceOfThePlot_vs_region.png\n",
      "SHAP Interaction Plot (mean split) for surfaceOfThePlot vs longitude saved to shap_outputs/shap_interaction_mean_surfaceOfThePlot_vs_longitude.png\n",
      "SHAP Interaction Plot (mean split) for surfaceOfThePlot vs energy_certificate saved to shap_outputs/shap_interaction_mean_surfaceOfThePlot_vs_energy_certificate.png\n",
      "SHAP Interaction Plot (mean split) for surfaceOfThePlot vs livingArea saved to shap_outputs/shap_interaction_mean_surfaceOfThePlot_vs_livingArea.png\n",
      "SHAP Interaction Plot (mean split) for energy_certificate vs region saved to shap_outputs/shap_interaction_mean_energy_certificate_vs_region.png\n",
      "SHAP Interaction Plot (mean split) for energy_certificate vs longitude saved to shap_outputs/shap_interaction_mean_energy_certificate_vs_longitude.png\n",
      "SHAP Interaction Plot (mean split) for energy_certificate vs surfaceOfThePlot saved to shap_outputs/shap_interaction_mean_energy_certificate_vs_surfaceOfThePlot.png\n",
      "SHAP Interaction Plot (mean split) for energy_certificate vs livingArea saved to shap_outputs/shap_interaction_mean_energy_certificate_vs_livingArea.png\n",
      "SHAP Interaction Plot (mean split) for livingArea vs region saved to shap_outputs/shap_interaction_mean_livingArea_vs_region.png\n",
      "SHAP Interaction Plot (mean split) for livingArea vs longitude saved to shap_outputs/shap_interaction_mean_livingArea_vs_longitude.png\n",
      "SHAP Interaction Plot (mean split) for livingArea vs surfaceOfThePlot saved to shap_outputs/shap_interaction_mean_livingArea_vs_surfaceOfThePlot.png\n",
      "SHAP Interaction Plot (mean split) for livingArea vs energy_certificate saved to shap_outputs/shap_interaction_mean_livingArea_vs_energy_certificate.png\n",
      "SHAP Interaction Plot (gain-based split) for livingArea vs locality saved to shap_outputs/shap_interaction_gain_livingArea_vs_locality.png\n",
      "SHAP Interaction Plot (gain-based split) for livingArea vs latitude saved to shap_outputs/shap_interaction_gain_livingArea_vs_latitude.png\n",
      "SHAP Interaction Plot (gain-based split) for livingArea vs longitude saved to shap_outputs/shap_interaction_gain_livingArea_vs_longitude.png\n",
      "SHAP Interaction Plot (gain-based split) for livingArea vs surfaceOfThePlot saved to shap_outputs/shap_interaction_gain_livingArea_vs_surfaceOfThePlot.png\n",
      "SHAP Interaction Plot (gain-based split) for locality vs livingArea saved to shap_outputs/shap_interaction_gain_locality_vs_livingArea.png\n",
      "SHAP Interaction Plot (gain-based split) for locality vs latitude saved to shap_outputs/shap_interaction_gain_locality_vs_latitude.png\n",
      "SHAP Interaction Plot (gain-based split) for locality vs longitude saved to shap_outputs/shap_interaction_gain_locality_vs_longitude.png\n",
      "SHAP Interaction Plot (gain-based split) for locality vs surfaceOfThePlot saved to shap_outputs/shap_interaction_gain_locality_vs_surfaceOfThePlot.png\n",
      "SHAP Interaction Plot (gain-based split) for latitude vs livingArea saved to shap_outputs/shap_interaction_gain_latitude_vs_livingArea.png\n",
      "SHAP Interaction Plot (gain-based split) for latitude vs locality saved to shap_outputs/shap_interaction_gain_latitude_vs_locality.png\n",
      "SHAP Interaction Plot (gain-based split) for latitude vs longitude saved to shap_outputs/shap_interaction_gain_latitude_vs_longitude.png\n",
      "SHAP Interaction Plot (gain-based split) for latitude vs surfaceOfThePlot saved to shap_outputs/shap_interaction_gain_latitude_vs_surfaceOfThePlot.png\n",
      "SHAP Interaction Plot (gain-based split) for longitude vs livingArea saved to shap_outputs/shap_interaction_gain_longitude_vs_livingArea.png\n",
      "SHAP Interaction Plot (gain-based split) for longitude vs locality saved to shap_outputs/shap_interaction_gain_longitude_vs_locality.png\n",
      "SHAP Interaction Plot (gain-based split) for longitude vs latitude saved to shap_outputs/shap_interaction_gain_longitude_vs_latitude.png\n",
      "SHAP Interaction Plot (gain-based split) for longitude vs surfaceOfThePlot saved to shap_outputs/shap_interaction_gain_longitude_vs_surfaceOfThePlot.png\n",
      "SHAP Interaction Plot (gain-based split) for surfaceOfThePlot vs livingArea saved to shap_outputs/shap_interaction_gain_surfaceOfThePlot_vs_livingArea.png\n",
      "SHAP Interaction Plot (gain-based split) for surfaceOfThePlot vs locality saved to shap_outputs/shap_interaction_gain_surfaceOfThePlot_vs_locality.png\n",
      "SHAP Interaction Plot (gain-based split) for surfaceOfThePlot vs latitude saved to shap_outputs/shap_interaction_gain_surfaceOfThePlot_vs_latitude.png\n",
      "SHAP Interaction Plot (gain-based split) for surfaceOfThePlot vs longitude saved to shap_outputs/shap_interaction_gain_surfaceOfThePlot_vs_longitude.png\n",
      "Exporting evaluation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/n2m_wdk56vqcbt6h62ljwtjc0000gn/T/ipykernel_14557/2529494967.py:563: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics and visualizations saved to model2.0__evaluation_metrics\n",
      "Evaluation completed in 129.54 seconds.\n",
      "Total script execution time: 203.62 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from rapidfuzz import process\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, income_path, zipcode_path, property_path):\n",
    "        self.income_path = income_path\n",
    "        self.zipcode_path = zipcode_path\n",
    "        self.property_path = property_path\n",
    "        \"\"\"\n",
    "        Preprocessing class:\n",
    "        This is where I match the Bpost location data and municipalities with the Statbel\n",
    "        Median income information.        \n",
    "\n",
    "        \"\"\"\n",
    "    def read_merge_external(self):\n",
    "        # Read income and zipcode data\n",
    "        income_data = pd.read_csv(self.income_path)\n",
    "        zipcode_data = pd.read_excel(self.zipcode_path)\n",
    "\n",
    "        # DEBUG: Print the columns of the input dataframes\n",
    "        print(\"Income Data Columns:\", income_data.columns)\n",
    "        print(\"Zipcode Data Columns:\", zipcode_data.columns)\n",
    "        #Clean Income Data\n",
    "        income_data_new_header = [\"Locality\", \"median_income\", \"unnamed\", \"GPS\", \"locality\"]\n",
    "        income_data.columns = income_data_new_header\n",
    "        income_data = income_data.drop(columns=[\"Locality\", \"unnamed\"])\n",
    "\n",
    "        # Ensure all column names are lowercase for consistent renaming\n",
    "        zipcode_data.columns = zipcode_data.columns.str.lower()\n",
    "\n",
    "        # Rename columns in the zipcode_data dataframe\n",
    "        if \"main municipality\" in zipcode_data.columns:\n",
    "            zipcode_data.rename(\n",
    "                columns={\"postcode\": \"postal_code\", \"provincie\": \"province\", \"name\": \"locality\", \"main municipality\": \"municipality\"},\n",
    "                inplace=True,\n",
    "            )\n",
    "        else:\n",
    "            raise KeyError(\"The column 'MAIN MUNICIPALITY' is missing from the input file. Please check the file structure.\")\n",
    "\n",
    "        # Normalize text for merging\n",
    "        zipcode_data.province = zipcode_data[\"province\"].astype(str)\n",
    "        zipcode_data.locality = zipcode_data[\"locality\"].astype(str)\n",
    "        zipcode_data.municipality = zipcode_data[\"municipality\"].astype(str)\n",
    "        zipcode_data.province = zipcode_data.province.apply(lambda x: x.strip().lower())\n",
    "        zipcode_data.locality = zipcode_data.locality.apply(lambda x: x.strip().lower())\n",
    "        zipcode_data.municipality = zipcode_data.municipality.apply(lambda x: x.strip().lower())\n",
    "        income_data.locality = income_data.locality.apply(lambda x: x.strip().lower())\n",
    "        income_data.locality = income_data[\"locality\"].astype(str)\n",
    "\n",
    "        # Merge postal code & province data from Bpost to income data\n",
    "        for index, row in income_data.iterrows():\n",
    "            id_locality = row[\"locality\"]\n",
    "            match = zipcode_data[zipcode_data[\"locality\"] == id_locality]\n",
    "            if not match.empty:\n",
    "                income_data.at[index, \"postal_code\"] = match[\"postal_code\"].values[0]\n",
    "                income_data.at[index, \"province\"] = match[\"province\"].values[0]\n",
    "\n",
    "        # RapidFuzz matching: For unmatched rows, use fuzzy matching\n",
    "        for index, row in income_data.iterrows():\n",
    "            if pd.isnull(row[\"postal_code\"]) or pd.isnull(row[\"province\"]):\n",
    "                locality = row[\"locality\"]\n",
    "                match = process.extractOne(locality, zipcode_data[\"locality\"], score_cutoff=75)\n",
    "                if match:\n",
    "                    match_row = zipcode_data[zipcode_data[\"locality\"] == match[0]]\n",
    "                    income_data.at[index, \"postal_code\"] = match_row[\"postal_code\"].values[0]\n",
    "                    income_data.at[index, \"province\"] = match_row[\"province\"].values[0]\n",
    "\n",
    "        return income_data, zipcode_data\n",
    "\n",
    "\n",
    "    def match_income(self, row, income_data, log_file=\"match_results.csv\", unmatched_file=\"unmatched_results.csv\"):\n",
    "        \"\"\"\n",
    "        Match income based on postal code or locality.\n",
    "        \n",
    "        Priority:\n",
    "        1. Exact match on postal code.\n",
    "        2. Fuzzy match on locality.\n",
    "        \n",
    "        If no match is found, return None.\n",
    "        Log each iteration to a CSV for double-checking.\n",
    "        \"\"\"\n",
    "        # Create a dictionary to log results for this row\n",
    "        log_data = {\n",
    "            \"postal_code\": row['postal_code'],\n",
    "            \"locality\": row['locality'],\n",
    "            \"province\": row['province'],\n",
    "            \"match_type\": \"None\",\n",
    "            \"matched_value\": None,\n",
    "            \"median_income\": None,\n",
    "            \"GPS\": None\n",
    "        }\n",
    "\n",
    "        # Attempt exact match on postal code\n",
    "        postal_matches = income_data[income_data['postal_code'] == row['postal_code']]\n",
    "        if not postal_matches.empty:\n",
    "            matched_income = postal_matches['median_income'].values[0]\n",
    "            matched_gps = postal_matches['GPS'].values[0]\n",
    "            log_data.update({\n",
    "                \"match_type\": \"Postal Code\",\n",
    "                \"matched_value\": row['postal_code'],\n",
    "                \"median_income\": matched_income,\n",
    "                \"GPS\": matched_gps\n",
    "            })\n",
    "            self._append_to_log(log_data, log_file)\n",
    "            return matched_income, matched_gps\n",
    "\n",
    "        \n",
    "        # Fuzzy match on locality\n",
    "        best_match = process.extractOne(row['locality'], income_data['locality'])\n",
    "        if best_match and best_match[1] > 75:  # Ensure the match score is above a threshold\n",
    "            matched_locality = best_match[0]\n",
    "            matched_income = income_data[income_data['locality'] == matched_locality]['median_income'].values[0]\n",
    "            matched_gps = income_data[income_data['locality'] == matched_locality]['GPS'].values[0]\n",
    "            log_data.update({\n",
    "                \"match_type\": \"Locality\",\n",
    "                \"matched_value\": matched_locality,\n",
    "                \"median_income\": matched_income,\n",
    "                \"GPS\": matched_gps\n",
    "            })\n",
    "            self._append_to_log(log_data, log_file)\n",
    "            return matched_income, matched_gps\n",
    "\n",
    "        # No match found; log to the unmatched file\n",
    "        self._append_to_log(log_data, unmatched_file)\n",
    "        return None\n",
    "\n",
    "    def _append_to_log(self, log_data, log_file):\n",
    "        \"\"\"\n",
    "        Append log data to the specified CSV file.\n",
    "        \"\"\"\n",
    "        # Convert the log data dictionary to a DataFrame\n",
    "        log_df = pd.DataFrame([log_data])\n",
    "\n",
    "        # Append to the CSV file\n",
    "        try:\n",
    "            # If the file exists, append without writing the header\n",
    "            log_df.to_csv(log_file, mode='a', index=False, header=False)\n",
    "        except FileNotFoundError:\n",
    "            # If the file does not exist, write with the header\n",
    "            log_df.to_csv(log_file, mode='w', index=False, header=True)\n",
    "\n",
    "    def properties_dataset_cleaning(self, income_data):\n",
    "        # Read and clean property data\n",
    "        property_data = pd.read_csv(self.property_path)\n",
    "        #Clear outliers above 5 million\n",
    "        property_data = property_data[\n",
    "            (property_data[\"price\"] <= 5000000) & (property_data[\"price\"] >= 40000)\n",
    "        ]\n",
    "        property_data = property_data[property_data[\"bedrooms\"] <= 9]\n",
    "        property_data[\"buildingState\"] = property_data[\"buildingState\"].replace(\n",
    "            {\n",
    "                \"AS_NEW\": 1,\n",
    "                \"JUST_RENOVATED\": 2,\n",
    "                \"GOOD\": 3,\n",
    "                \"TO_RESTORE\": 4,\n",
    "                \"TO_RENOVATE\": 4,\n",
    "                \"TO_BE_DONE_UP\": 4,\n",
    "            }\n",
    "        )\n",
    "        property_data[\"province\"] = property_data[\"province\"].replace(\n",
    "            {\n",
    "                \"flemish_brabant_extended\": \"flemish_brabant\",\n",
    "                \"hainaut_extended\": \"hainaut_province\",\n",
    "                \"flemish_brabant\": \"Flemish Brabant\",\n",
    "                \"hainaut_province\": \"Hainaut\",\n",
    "            }\n",
    "        )\n",
    "        property_data.drop(columns=[\"buildingStateLabel\"], inplace=True)\n",
    "\n",
    "        # Fill and clean missing values\n",
    "        property_data[\"terraceSurface\"] = property_data[\"terraceSurface\"].fillna(0)\n",
    "        property_data.dropna(subset=[\"livingArea\", \"energy_certificate\"], inplace=True)\n",
    "        # Normalize locality column\n",
    "        property_data[\"locality\"] = property_data[\"locality\"].apply(lambda x: x.strip().lower())\n",
    "\n",
    "         # Add median_income and GPS columns\n",
    "        property_data[['median_income', 'GPS']] = property_data.apply(\n",
    "        lambda row: pd.Series(self.match_income(row, income_data)), \n",
    "        axis=1\n",
    "        )\n",
    "    \n",
    "         # Export updated property data\n",
    "        self.export_property_data(property_data)\n",
    "\n",
    "        return property_data\n",
    "\n",
    "    def export_property_data(self, property_data, base_filename='properties_data_cleaned'):\n",
    "        \"\"\"\n",
    "        Export the processed property data to a CSV file.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create exports directory if it doesn't exist\n",
    "        exports_dir = os.path.join(os.path.dirname(self.property_path), 'exports')\n",
    "        os.makedirs(exports_dir, exist_ok=True)\n",
    "\n",
    "        # Generate filename with timestamp\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{base_filename}_{timestamp}.csv\"\n",
    "        full_path = os.path.join(exports_dir, filename)\n",
    "\n",
    "        # Export to CSV\n",
    "        property_data.to_csv(full_path, index=False)\n",
    "        \n",
    "        print(f\"Property data exported to: {full_path}\")\n",
    "        \n",
    "        return full_path\n",
    "\n",
    "class FeatureEngineering:\n",
    "    @staticmethod\n",
    "    def add_region_column(df):\n",
    "        flanders_provinces = [\n",
    "            \"Antwerp\",\n",
    "            \"East Flanders\",\n",
    "            \"Flemish Brabant\",\n",
    "            \"Limburg\",\n",
    "            \"West Flanders\",\n",
    "        ]\n",
    "        wallonia_provinces = [\n",
    "            \"Liège\",\n",
    "            \"Luxembourg\",\n",
    "            \"Walloon Brabant\",\n",
    "            \"Namur\",\n",
    "            \"Hainaut\",\n",
    "        ]\n",
    "        df[\"region\"] = df[\"province\"].apply(\n",
    "            lambda province: \"Flanders\"\n",
    "            if province in flanders_provinces\n",
    "            else \"Wallonia\"\n",
    "            if province in wallonia_provinces\n",
    "            else \"Brussels\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def split_gps_coordinates(df):\n",
    "        \"\"\"\n",
    "        Splits the GPS coordinates column into latitude and longitude.\n",
    "        \"\"\"\n",
    "        df[['latitude', 'longitude']] = df['GPS'].str.split(' ', expand=True).astype(float)\n",
    "        df.drop(columns=[\"GPS\"], inplace=True)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_median_income(df):\n",
    "        df[\"median_income\"]=df[\"median_income\"].apply(lambda x: float(x) * 1000)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_outliers_iqr(df):\n",
    "        # Interquartile Range (IQR) Method\n",
    "        q1 = df[\"price\"].quantile(0.25)\n",
    "        q3 = df[\"price\"].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df = df[(df[\"price\"] >= q1 - 1.5 * iqr) & (df[\"price\"] <= q3 + 1.5 * iqr)]\n",
    "        return df\n",
    "\n",
    "\n",
    "class ModelApply:\n",
    "    @staticmethod\n",
    "    def train_model(df):\n",
    "        # Prepare data for training\n",
    "        X = df.drop(\n",
    "            columns=[\n",
    "                \"price\",\n",
    "                \"kitchen\",\n",
    "                \"postal_code\",\n",
    "                \"furnished\",\n",
    "                \"fireplace\",\n",
    "                \"province\",\n",
    "                \"property_type\",\n",
    "                \"terraceSurface\",\n",
    "            ]\n",
    "        )\n",
    "        y = df[\"price\"]\n",
    "\n",
    "        # Categorical features\n",
    "        categorical_features = [\"locality\", \"energy_certificate\", \"region\"]\n",
    "        \n",
    "        \n",
    "        # KFold cross-validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        feature_importances = []\n",
    "\n",
    "        params = {\n",
    "            \"random_strength\": 5,\n",
    "            \"learning_rate\": 0.07,\n",
    "            \"l2_leaf_reg\": 7,\n",
    "            \"iterations\": 1500,\n",
    "            \"depth\": 6,\n",
    "            \"eval_metric\": \"RMSE\",\n",
    "            \"verbose\": 100,\n",
    "            \"random_seed\": 42,\n",
    "            \"cat_features\": categorical_features,\n",
    "        }\n",
    "\n",
    "        # Train with KFold\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            model = CatBoostRegressor(**params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
    "            feature_importances.append(model.get_feature_importance())\n",
    "            model.save_model(\"catboost_model_2.0.cbm\")\n",
    "\n",
    "        return model, X_train, X_test, y_train, y_test, feature_importances\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Functions for MAPE and sMAPE\n",
    "        def mape(y_true, y_pred):\n",
    "            \"\"\"Mean Absolute Percentage Error\"\"\"\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        def smape(y_true, y_pred):\n",
    "            \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "            return 100 * np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "        \n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            \"MAE_train\": mean_absolute_error(y_train, y_train_pred),\n",
    "            \"MAE_test\": mean_absolute_error(y_test, y_test_pred),\n",
    "            \"RMSE_train\": np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "            \"RMSE_test\": np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "            \"R2_train\": r2_score(y_train, y_train_pred),\n",
    "            \"R2_test\": r2_score(y_test, y_test_pred),\n",
    "            \"MAPE_train\": mape(y_train, y_train_pred),\n",
    "            \"MAPE_test\": mape(y_test, y_test_pred),\n",
    "            \"sMAPE_train\": smape(y_train, y_train_pred),\n",
    "            \"sMAPE_test\": smape(y_test, y_test_pred),\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def shap_analysis(model, X_test, y_test, categorical_features):\n",
    "        \"\"\"\n",
    "        Performs SHAP analysis on the model to understand feature contributions and feature interactions.\n",
    "\n",
    "        Includes both gain-based and mean split importance analyses.\n",
    "\n",
    "        Outputs:\n",
    "            SHAP summary plot, dependence plots, and interaction plots (both gain-based and mean split) are saved as images.\n",
    "        \"\"\"\n",
    "        # Create a test Pool with categorical features\n",
    "        test_pool = Pool(X_test, y_test, cat_features=categorical_features)\n",
    "\n",
    "        # Initialize SHAP TreeExplainer\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(test_pool)\n",
    "\n",
    "        # Create directory to store SHAP outputs\n",
    "        shap_output_dir = \"shap_outputs\"\n",
    "        os.makedirs(shap_output_dir, exist_ok=True)\n",
    "\n",
    "        # Plotting to prevent cutoff\n",
    "        plt.rcParams.update({\n",
    "            'figure.autolayout': True,\n",
    "            'figure.figsize': (12, 8),  # Larger figure size\n",
    "            'figure.constrained_layout.use': True,\n",
    "        })\n",
    "\n",
    "        # SHAP Summary Plot (global feature importance, mean split)\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        shap.summary_plot(shap_values, X_test, show=False, plot_type='bar')\n",
    "        plt.title(\"SHAP Summary Plot (Mean Split)\")\n",
    "        plt.tight_layout(pad=3.0)  # Add extra padding\n",
    "        plt.savefig(f\"{shap_output_dir}/shap_summary_plot_mean.png\", bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"SHAP Summary Plot (mean split) saved to {shap_output_dir}/shap_summary_plot_mean.png\")\n",
    "\n",
    "        # Gain-Based Feature Importances\n",
    "        gain_importances = model.get_feature_importance(type='PredictionValuesChange')\n",
    "        gain_importance_indices = np.argsort(gain_importances)[::-1]  # Sort in descending order\n",
    "        gain_top_features = [X_test.columns[i] for i in gain_importance_indices[:5]]  # Top 5 features\n",
    "\n",
    "        # SHAP Summary Plot for Gain-Based Importance\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        shap.summary_plot(shap_values, X_test, show=False, plot_type=\"bar\")\n",
    "        plt.title(\"SHAP Summary Plot (Gain-Based)\")\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.savefig(f\"{shap_output_dir}/shap_summary_plot_gain.png\", bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"SHAP Summary Plot (gain-based split) saved to {shap_output_dir}/shap_summary_plot_gain.png\")\n",
    "\n",
    "        # Generate SHAP Dependence Plots (Mean Split)\n",
    "        for feature in X_test.columns:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.dependence_plot(feature, shap_values, X_test, show=False)\n",
    "            plt.title(f\"SHAP Dependence Plot: {feature} (Mean Split)\")\n",
    "            plt.tight_layout(pad=3.0)\n",
    "            plt.savefig(f\"{shap_output_dir}/shap_dependence_mean_{feature}.png\", bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"SHAP Dependence Plot (mean split) for {feature} saved to {shap_output_dir}/shap_dependence_mean_{feature}.png\")\n",
    "\n",
    "        # Generate SHAP Dependence Plots (Gain-Based)\n",
    "        for feature in gain_top_features:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.dependence_plot(feature, shap_values, X_test, show=False)\n",
    "            plt.title(f\"SHAP Dependence Plot: {feature} (Gain-Based)\")\n",
    "            plt.tight_layout(pad=3.0)\n",
    "            plt.savefig(f\"{shap_output_dir}/shap_dependence_gain_{feature}.png\", bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"SHAP Dependence Plot (gain-based split) for {feature} saved to {shap_output_dir}/shap_dependence_gain_{feature}.png\")\n",
    "\n",
    "        # Generate SHAP Interaction Plots (Mean Split)\n",
    "        mean_split_top_features_indices = np.argsort(np.abs(shap_values).mean(0))[-5:]  # Top 5 features by mean split\n",
    "        mean_split_top_features = [X_test.columns[i] for i in mean_split_top_features_indices]\n",
    "\n",
    "        for i, feature_x in enumerate(mean_split_top_features):\n",
    "            for j, feature_y in enumerate(mean_split_top_features):\n",
    "                if i != j:  # Avoid self-interactions\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    shap.dependence_plot(feature_x, shap_values, X_test, show=False, interaction_index=feature_y)\n",
    "                    plt.title(f\"SHAP Interaction Plot: {feature_x} vs {feature_y} (Mean Split)\")\n",
    "                    plt.tight_layout(pad=3.0)\n",
    "                    plt.savefig(f\"{shap_output_dir}/shap_interaction_mean_{feature_x}_vs_{feature_y}.png\", bbox_inches='tight', dpi=300)\n",
    "                    plt.close()\n",
    "                    print(f\"SHAP Interaction Plot (mean split) for {feature_x} vs {feature_y} saved to {shap_output_dir}/shap_interaction_mean_{feature_x}_vs_{feature_y}.png\")\n",
    "\n",
    "        # Generate SHAP Interaction Plots (Gain-Based)\n",
    "        for i, feature_x in enumerate(gain_top_features):\n",
    "            for j, feature_y in enumerate(gain_top_features):\n",
    "                if i != j:  # Avoid self-interactions\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    shap.dependence_plot(feature_x, shap_values, X_test, show=False, interaction_index=feature_y)\n",
    "                    plt.title(f\"SHAP Interaction Plot: {feature_x} vs {feature_y} (Gain-Based)\")\n",
    "                    plt.tight_layout(pad=3.0)\n",
    "                    plt.savefig(f\"{shap_output_dir}/shap_interaction_gain_{feature_x}_vs_{feature_y}.png\", bbox_inches='tight', dpi=300)\n",
    "                    plt.close()\n",
    "                    print(f\"SHAP Interaction Plot (gain-based split) for {feature_x} vs {feature_y} saved to {shap_output_dir}/shap_interaction_gain_{feature_x}_vs_{feature_y}.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_training_validation_loss(model, output_dir):\n",
    "        \"\"\"\n",
    "        Plot training and validation loss across iterations\n",
    "        \n",
    "        Args:\n",
    "            model (CatBoostRegressor): Trained CatBoost model\n",
    "            output_dir (str): Directory to save the plot\n",
    "        \"\"\"\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Extract learning curves\n",
    "        train_loss = model.get_evals_result()['learn']['RMSE']\n",
    "        validation_loss = model.get_evals_result()['validation']['RMSE']\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss (RMSE)', color='blue')\n",
    "        plt.plot(range(1, len(validation_loss) + 1), validation_loss, label='Validation Loss (RMSE)', color='red')\n",
    "        \n",
    "        plt.title('Training vs Validation Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(os.path.join(output_dir, 'model2.0_training_validation_loss.png'))\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_prediction_scatter(y_test, y_pred, output_dir):\n",
    "        \"\"\"\n",
    "        Create a scatter plot of actual vs predicted values with linear regression line\n",
    "        \n",
    "        \"\"\"\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5, color='blue', label='Predictions')\n",
    "        \n",
    "        # Compute linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(y_test, y_pred)\n",
    "        line = slope * y_test + intercept\n",
    "        \n",
    "        # Plot regression line\n",
    "        plt.plot(y_test, line, color='red', label=f'Regression Line (R²: {r_value**2:.4f})')\n",
    "        \n",
    "        plt.title('Actual vs Predicted Values')\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(os.path.join(output_dir, 'model2.0_actual_vs_predicted_scatter.png'))\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def export_evaluation_results(metrics, feature_importances, X_train, file_path, model=None, y_test=None, y_pred=None, export_type=\"csv\"):\n",
    "        \"\"\"\n",
    "        Extended export method to include additional visualizations\n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        # Create evaluation metrics directory\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "        evaluation_metrics_dir = os.path.join(output_dir, 'model2.0__evaluation_metrics')\n",
    "        os.makedirs(evaluation_metrics_dir, exist_ok=True)\n",
    "\n",
    "        # Create SHAP outputs directory\n",
    "        shap_output_dir = os.path.join(evaluation_metrics_dir, 'model2.0__shap_outputs')\n",
    "        os.makedirs(shap_output_dir, exist_ok=True)\n",
    "\n",
    "        # Save Metrics\n",
    "        metrics_file_path = os.path.join(evaluation_metrics_dir, 'model2.0__evaluation_results.csv')\n",
    "        if export_type == \"csv\":\n",
    "            pd.DataFrame([metrics]).to_csv(metrics_file_path, index=False)\n",
    "        elif export_type in (\"txt\", \"md\"):\n",
    "            with open(metrics_file_path, \"w\") as f:\n",
    "                f.write(\"# Model Evaluation Results\\n\\n\" if export_type == \"md\" else \"\")\n",
    "                for key, value in metrics.items():\n",
    "                    f.write(f\"- **{key}**: {value}\\n\" if export_type == \"md\" else f\"{key}: {value}\\n\")\n",
    "\n",
    "        # Dynamically retrieve feature names\n",
    "        feature_names = X_train.columns.tolist()\n",
    "\n",
    "        # Aggregate feature importances by taking the mean across folds\n",
    "        mean_feature_importances = (\n",
    "            np.mean(feature_importances, axis=0) if isinstance(feature_importances, list) else feature_importances\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame for feature importances\n",
    "        feature_importance_df = pd.DataFrame(\n",
    "            {\"Feature\": feature_names, \"Importance\": mean_feature_importances}\n",
    "        ).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "        # Save Feature Importances Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df, palette=\"viridis\")\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Features\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(evaluation_metrics_dir, 'model2.0__feature_importances.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Additional Visualizations (if model and predictions are provided)\n",
    "        if model is not None:\n",
    "            # Plot Training vs Validation Loss\n",
    "            ModelEvaluation.plot_training_validation_loss(model, evaluation_metrics_dir)\n",
    "        \n",
    "        if y_test is not None and y_pred is not None:\n",
    "            # Plot Actual vs Predicted Scatter\n",
    "            ModelEvaluation.plot_prediction_scatter(y_test, y_pred, evaluation_metrics_dir)\n",
    "\n",
    "        print(f\"Evaluation metrics and visualizations saved to {evaluation_metrics_dir}\")\n",
    "\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Paths\n",
    "    script_start_time = time.time()\n",
    "    print(\"Script started.\")\n",
    "    income_path = \"/Users/irisvirus/Desktop/Becode/Python/Projects/Deployment/Immo_deployment/utils/INCOME DATA 2022.csv\"\n",
    "    zipcode_path = \"/Users/irisvirus/Desktop/Becode/Python/Projects/Deployment/Immo_deployment/utils/zipcodes_num_nl_new_Tumi.xls\"\n",
    "    property_path = \"/Users/irisvirus/Desktop/Becode/Python/Projects/Deployment/Immo_deployment/utils/properties_data_cleaned_05_12_14H30.csv\"\n",
    "\n",
    "    # Preprocessing\n",
    "    preprocessing = Preprocessing(income_path, zipcode_path, property_path)\n",
    "    income_data, zipcode_data = preprocessing.read_merge_external()\n",
    "    property_data = preprocessing.properties_dataset_cleaning(income_data)\n",
    "\n",
    "    # Feature Engineering\n",
    "    feature_engineering = FeatureEngineering()\n",
    "    property_data = feature_engineering.split_gps_coordinates(property_data)\n",
    "    property_data = feature_engineering.scale_median_income(property_data)\n",
    "    property_data = feature_engineering.add_region_column(property_data)\n",
    "    property_data = feature_engineering.remove_outliers_iqr(property_data)\n",
    "    preprocessing.export_property_data(property_data)\n",
    "\n",
    "\n",
    "    # Model Training\n",
    "    training_start_time = time.time()\n",
    "    print(\"Model training started...\")\n",
    "    model_apply = ModelApply()\n",
    "    model, X_train, X_test, y_train, y_test, feature_importances = model_apply.train_model(property_data)\n",
    "    \n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    print(f\"Model training completed in {training_duration:.2f} seconds.\")\n",
    "    # Model Evaluation\n",
    "    evaluation_start_time = time.time()\n",
    "    print(\"Evaluation started...\")\n",
    "    evaluation = ModelEvaluation()\n",
    "    metrics = ModelEvaluation.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "    # SHAP Analysis\n",
    "    ModelEvaluation.shap_analysis(model, X_test, y_test, [\"locality\", \"energy_certificate\", \"region\"])\n",
    "    \n",
    "    #Gain based importance\n",
    "    #gain_importances = evaluation.calculate_gain_importance(model)\n",
    "    # y_test_pred for scatter plot and loss curves \n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Export Evaluation Results\n",
    "    print(\"Exporting evaluation results...\")\n",
    "    file_path = \"evaluation_results\"  # Replace with your desired output directory\n",
    "    evaluation.export_evaluation_results(\n",
    "        metrics=metrics,\n",
    "        feature_importances=feature_importances,\n",
    "        X_train=X_train,\n",
    "        file_path=file_path,\n",
    "        model=model,\n",
    "        y_test=y_test,\n",
    "        y_pred=y_test_pred,\n",
    "        export_type=\"csv\",\n",
    "    )\n",
    "    \n",
    "    time.sleep(2)  \n",
    "    evaluation_end_time = time.time()\n",
    "    # Evaluation duration\n",
    "    evaluation_duration = evaluation_end_time - evaluation_start_time\n",
    "    print(f\"Evaluation completed in {evaluation_duration:.2f} seconds.\")\n",
    "\n",
    "    # Capture the script end time\n",
    "    script_end_time = time.time()\n",
    "\n",
    "    # Total script execution time\n",
    "    script_execution_time = script_end_time - script_start_time\n",
    "    print(f\"Total script execution time: {script_execution_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
